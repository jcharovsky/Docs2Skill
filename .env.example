# LLM Configuration for SKILL.md Generation
# Copy this file to .env and fill in your values

# LLM Provider
# Supported: 'anthropic', 'openai', 'openrouter', 'gemini', 'grok', 'ollama'
LLM_PROVIDER=anthropic

# Your API Key (not required for Ollama)
LLM_API_KEY=your_api_key_here

# Model name to use
# See full list below for each provider
LLM_MODEL=claude-3-5-sonnet-20241022

# API Endpoint (optional - defaults are provided for each provider)
# Only specify if you need to override the default
# LLM_ENDPOINT=

# ============================================================================
# SUPPORTED MODELS BY PROVIDER
# ============================================================================

# ANTHROPIC
# Models: claude-3-5-sonnet-20241022, claude-3-5-sonnet-20240620,
#         claude-3-opus-20240229, claude-3-sonnet-20240229,
#         claude-3-haiku-20240307
# Default endpoint: https://api.anthropic.com/v1/messages

# OPENAI
# Models: gpt-4, gpt-4-turbo, gpt-4-turbo-preview, gpt-4o, gpt-4o-mini,
#         gpt-3.5-turbo, gpt-3.5-turbo-16k
# Default endpoint: https://api.openai.com/v1/chat/completions

# GOOGLE GEMINI
# Models: gemini-2.0-flash-exp, gemini-1.5-pro, gemini-1.5-flash,
#         gemini-1.0-pro
# Default endpoint: https://generativelanguage.googleapis.com/v1beta/models/
# Note: Requires API key from https://makersuite.google.com/app/apikey

# GROK (xAI)
# Models: grok-beta, grok-2-1212, grok-2-vision-1212
# Default endpoint: https://api.x.ai/v1/chat/completions
# Note: Requires API key from https://console.x.ai

# OPENROUTER
# Models: anthropic/claude-3.5-sonnet, openai/gpt-4, google/gemini-pro,
#         meta-llama/llama-3-70b-instruct, mistralai/mixtral-8x7b-instruct,
#         and many more (see https://openrouter.ai/models)
# Default endpoint: https://openrouter.ai/api/v1/chat/completions

# OLLAMA (Local)
# Models: llama3, llama3:70b, llama2, mistral, mixtral, codellama, phi,
#         deepseek-coder, qwen, gemma, neural-chat, starling-lm,
#         and many more (see https://ollama.com/library)
# Default endpoint: http://localhost:11434/v1/chat/completions
# Note: No API key required. Requires Ollama running locally.
# Install: https://ollama.com/download
